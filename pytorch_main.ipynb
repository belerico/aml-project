{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 378kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from torchtext) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from torchtext) (1.4.0+cpu)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from requests->torchtext) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from requests->torchtext) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from requests->torchtext) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/belerico/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages (4.41.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m\u001b[22mShell for\u001b[39m\u001b[22m \u001b[32m\u001b[1m/home/belerico/.local/share/virtualenvs/aml-project-EQl709OG\u001b[39m\u001b[22m \u001b[39m\u001b[1malready activated.\u001b[39m\u001b[22m\r\n",
      "No action taken to avoid nested environments.\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pipenv shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "non_alphanum_regex = re.compile(r'\\W+')\n",
    "\n",
    "def preprocess(doc, method='nltk', dataset=True):\n",
    "    if method == 'spacy':\n",
    "        tokens = \" \".join(\n",
    "            [\n",
    "                token.lower_\n",
    "                for token in doc\n",
    "                if token\n",
    "                and not (token.lower_ == \"null\" or token.is_stop or token.is_punct)\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'nltk':\n",
    "        # doc = non_alphanum_regex.sub(' ', doc).lower()\n",
    "        tokens = [\n",
    "                token\n",
    "                for token in word_tokenize(doc.lower())\n",
    "                if not (token == \"null\" or token in english_stopwords or token in string.punctuation)\n",
    "            ]\n",
    "    elif method == 'keras':\n",
    "        tokens = \" \".join(\n",
    "            [\n",
    "                token\n",
    "                for token in text_to_word_sequence(doc)\n",
    "                if not (token == \"null\" or token in english_stopwords or token in string.punctuation)\n",
    "            ]\n",
    "        )\n",
    "    if dataset or tokens != \"\":\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def parse_content_line(x, attributes=None, label=True):\n",
    "    if attributes is None:\n",
    "        attributes = [\"title_left\", \"title_right\"]\n",
    "    item = json.loads(x)\n",
    "    elements = [item[attr] if item[attr] is not None else '' for attr in attributes]\n",
    "    if label:\n",
    "        elements.append(int(item[\"label\"]))\n",
    "    item = np.array(elements)\n",
    "    return item[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=preprocess, lower=True, fix_length=20, batch_first=True, pad_token='0')\n",
    "LABEL = Field(sequential=False, use_vocab=False, is_target=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import pandas\n",
    "\n",
    "contents = []\n",
    "for i, x in enumerate(open('./dataset/computers/train/computers_splitted_train_medium.json', \"r\").readlines()):\n",
    "    try:\n",
    "        item = parse_content_line(x, attributes=None, label=True)\n",
    "        contents.append(item)\n",
    "    except:\n",
    "        print(\"Lost data at line {}\".format(i))\n",
    "\n",
    "contents = np.concatenate(contents, axis=0)\n",
    "train = pandas.DataFrame(data=contents, columns=['title_left', 'title_right', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import pandas\n",
    "\n",
    "contents = []\n",
    "for i, x in enumerate(open('./dataset/computers/valid/computers_splitted_valid_medium.json', \"r\").readlines()):\n",
    "    try:\n",
    "        item = parse_content_line(x, attributes=None, label=True)\n",
    "        contents.append(item)\n",
    "    except:\n",
    "        print(\"Lost data at line {}\".format(i))\n",
    "\n",
    "contents = np.concatenate(contents, axis=0)\n",
    "valid = pandas.DataFrame(data=contents, columns=['title_left', 'title_right', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Dataset, Example\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
    "    def __init__(self, examples, fields, filter_pred=None):\n",
    "        \"\"\"\n",
    "        Create a dataset from a pandas dataframe of examples and Fields\n",
    "        Arguments:\n",
    "            examples pd.DataFrame: DataFrame of examples\n",
    "            fields {str: Field}: The Fields to use in this tuple. The\n",
    "                string is a field name, and the Field is the associated field.\n",
    "            filter_pred (callable or None): use only exanples for which\n",
    "                filter_pred(example) is true, or use all examples if None.\n",
    "                Default is None\n",
    "        \"\"\"\n",
    "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
    "        if filter_pred is not None:\n",
    "            self.examples = filter(filter_pred, self.examples)\n",
    "        self.fields = dict(fields)\n",
    "        # Unpack field tuples\n",
    "        for n, f in list(self.fields.items()):\n",
    "            if isinstance(n, tuple):\n",
    "                self.fields.update(zip(n, f))\n",
    "                del self.fields[n]\n",
    "        \n",
    "                \n",
    "class SeriesExample(Example):\n",
    "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
    "    @classmethod\n",
    "    def fromSeries(cls, data, fields):\n",
    "        return cls.fromdict(data.to_dict(), fields)\n",
    "    \n",
    "    @classmethod\n",
    "    def fromdict(cls, data, fields):\n",
    "        ex = cls()\n",
    "        for key, field in fields.items():\n",
    "            if key not in data:\n",
    "                raise ValueError(\"Specified key {} was not found in \"\n",
    "                \"the input data\".format(key))\n",
    "            if field is not None:\n",
    "                setattr(ex, key, field.preprocess(data[key]))\n",
    "            else:\n",
    "                setattr(ex, key, data[key])\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fields = {\"title_left\": TEXT, 'title_right': TEXT, 'label': LABEL}\n",
    "train_ds = DataFrameDataset(train, fields)\n",
    "valid_ds = DataFrameDataset(valid, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asus', 'prime', 'x299', 'deluxe', 'prijzen', 'tweakers']\n",
      "['495906', 'b21', 'hp', 'x5560', '2', '80ghz', 'ml350', 'g6', 'new', 'wholesale', 'price']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0].title_left)\n",
    "print(valid_ds[0].title_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hp', 4006),\n",
       " ('2', 3293),\n",
       " ('3', 3033),\n",
       " ('gb', 2584),\n",
       " ('5', 2269),\n",
       " ('b21', 2149),\n",
       " ('com', 2030),\n",
       " ('price', 1979),\n",
       " ('wholesale', 1912),\n",
       " ('core', 1809)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    "     (train_ds, valid_ds), # we pass in the datasets we want the iterator to draw data from\n",
    "     batch_sizes=(32, 32),\n",
    "     device=torch.device('cpu'), # if you want to use the GPU, specify the GPU number here\n",
    "     sort_key=lambda x: min(max(len(x.title_left), len(x.title_right)), 20), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "     sort_within_batch=True,\n",
    "     repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_vars, y_var):\n",
    "        self.dl, self.x_vars, self.y_var = dl, x_vars, y_var # we pass in the list of attributes for x \n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            left = getattr(batch, self.x_vars[0]) # we assume only one input in this wrapper\n",
    "            right = getattr(batch, self.x_vars[1]) # we assume only one input in this wrapper\n",
    "            y = torch.Tensor(list(map(float, getattr(batch, self.y_var))))\n",
    "\n",
    "            yield (left, right, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, ['title_left', 'title_right'], 'label')\n",
    "valid_dl = BatchWrapper(val_iter, ['title_left', 'title_right'], 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  12,  304,  699,  432,    5, 1283,  606,  300, 2030,  408,   38,  757,\n",
       "           480,    3,    6,  211,   30, 2243,    1,    1],\n",
       "         [2603,   49,    2,  252,    5,  180,   62,    3,    6,  382,   71,   13,\n",
       "            43,   10,    9,  208,    1,    1,    1,    1],\n",
       "         [  64,  186,  376, 1288,  926,  410,   74,   29,   40,    5,  299, 2085,\n",
       "           569,  326,  173,  285,  101,    8,    1,    1],\n",
       "         [  19,   84,   26,  107,    4,   11,   24,   36,    5,   81,  222,   42,\n",
       "           790,  795,  157,  153,  101,    8,    1,    1],\n",
       "         [  96,   23,  378,  253,  395,  197,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [1434, 2306,  187, 3154,  291,  358,  232,  128,  318,   50, 1853, 3782,\n",
       "           218, 3536, 3660, 3587,  321,  153,    1,    1],\n",
       "         [  42,   23,  237,    4,    6,   15,  729,  216, 2060,  181,  362,  361,\n",
       "           223,  307,  392,  317,  354,  352,    1,    1],\n",
       "         [  19,  107,    4,   50,   84,   26,    3,  351,   12,   11,   24,   54,\n",
       "           211,  157,  127,  241, 2016, 1202,    1,    1],\n",
       "         [  19,  177,   77, 2162,   11,   24,    3,   38,  118,   40,    5,   30,\n",
       "           125, 1792,   78,  153,   28,    8,    1,    1],\n",
       "         [   2, 1263,  261,   23,  168, 2457,  986,  480,  215,   38,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [ 129,  267,  977,  776,  541,  215,   38,  391,  797,   36,    5,   81,\n",
       "            88, 1502,  157,  153,   28,    8,    1,    1],\n",
       "         [1410,  621,   99,   55,   56,   91,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [   2,  846,   60,  403,  415, 1183,   75,  192,  584, 1685, 1641,  778,\n",
       "             7,  552,  146,  189,  556,  555,    1,    1],\n",
       "         [  41,   33,  164,    4,  616, 1056,  508,  161,  212,  441,  133,  409,\n",
       "           738,  155,  217,  219,  220,  204,    1,    1],\n",
       "         [1037,  770,  596,    5,   62,    3,    6,   71,   42,   13,   10,    9,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  41, 1080,  273,   38,  853,   11,  503,   23,   83,    4,  811,   63,\n",
       "           297,   33,  114, 1745, 1473,   94,    1,    1],\n",
       "         [  41, 1256, 1376,  520,   33,   54,   11, 3080, 1956,   32,   22,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [1313,    7,    2,  163,  196,  594,    5,  180,    4,    6,   15,  657,\n",
       "            34,   13,   43,   10,    9,  208,    1,    1],\n",
       "         [ 129, 2391,  245, 1115, 1419,  111,   16,   30,  125,   15,   80, 1724,\n",
       "            55,   56,   91,  248,  101,    8,    1,    1],\n",
       "         [  64,   73,   68,  226,  829,  247,   74,   29, 1208,   40,    5,  299,\n",
       "          1668,  569,  173,  285,   28,    8,    1,    1],\n",
       "         [ 908, 1090,   26, 2610,  456,   24,   17,   89,   34,  680,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [   2, 1012,  473, 1692,  123,  292,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  19,  107,    4,   50,   84,   26,    3,  351,   12,   11,   24,   54,\n",
       "           211,  157,  127,  241, 2016, 1202,    1,    1],\n",
       "         [  19,   84,   26,  107,    4,   11,   24,   36,    5,   81,  222,   42,\n",
       "           790,  795,  157,  153,   28,    8,    1,    1],\n",
       "         [ 607,  254,   51,  462,  158,  149,   40,    5, 1162, 2314, 1803,   23,\n",
       "           482,  146,    3,    6,  450,  528,    1,    1],\n",
       "         [ 723,  344, 1414,   35, 2552,  536, 1916, 1254, 2525, 2446,  505,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  41,  714,  402, 1462,   23,   83,  586,    4, 1198,  175,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [ 405,   73,   68,  226,  119,   82,  410,  782,  137,   70,  138,   74,\n",
       "            29,  904,   20,    1,    1,    1,    1,    1],\n",
       "         [   3,    6,   50,  533,  730,   12,  276,  534,  826,   55,   56,   16,\n",
       "            34, 2187, 1085,   32,   22,    1,    1,    1],\n",
       "         [  14,   47,  104,   23,   48,  313,   31,  102,  436,  324,   54,  120,\n",
       "            37,  745,   94,    1,    1,    1,    1,    1],\n",
       "         [  14,   47,  104,   31,   88,    5,    3,   82,   40,   58,  249,   75,\n",
       "           745,  172,   27,   81,  101,    8,    1,    1],\n",
       "         [  14,  124,   69,  651,  128,  294,  312,   83,  271,  577,  546,  277,\n",
       "           400,   43,  306, 2043,  879,   94,    1,    1]]),\n",
       " tensor([[ 103,  304,    3,    6,   55,   56,   16,  202, 1871,   67,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [1150,  367,    2,  163,  196,  440,    5,  100,    3,    6,   71, 1083,\n",
       "            34,   13,   43,   10,    9,  171,    1,    1],\n",
       "         [  64,  110,   73,   68,  226,  829,  247, 2407,  299, 1668,  569,   32,\n",
       "            22,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  19,   84,   26,  107,   50, 1088,  309,   61,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  98,  323,    4,    6, 1572,  670,   55,   56, 1544,   16,   17,  657,\n",
       "          1411,  216,  133, 3077,   32,   22,    1,    1],\n",
       "         [ 136, 1834,  165,  936,  218,  239,  446, 1117,  244, 3076,   32,   22,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  42,   23,  237,    4,    6,   15,  729,  216, 4075,  362,  361,  223,\n",
       "           307,  392,  317,  354,  352,    1,    1,    1],\n",
       "         [  19,   84,   26,  107,    4,   24,  157,  127,  795,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  19,  177,   77,    3,  351,   12,   11,   24,  127, 1408,  284,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [ 465,  541,  215, 3735, 1389, 1388,   38,  391,  622,   36,    5,   81,\n",
       "            88,   34, 1688, 1038, 1026,  123,    1,    1],\n",
       "         [ 129, 1049,  406,  850,   12,    3,  508,   21, 2130,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  12,  730,  380,  826,   69,   15,   80, 2343,  657,    3,    6,  533,\n",
       "            99,   55,   56,   16,  458, 1085,    1,    1],\n",
       "         [ 888,    7,    2,   17,  315,  147, 1368,   85,   37,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  41,  164,    4, 1056,  508,   79,   11,   63,  369,  234,   33, 1497,\n",
       "           321,  153,    1,    1,    1,    1,    1,    1],\n",
       "         [1150,  367,    2,  163,  196,  440,    5,  100,    3,    6,   71, 1083,\n",
       "            34,   13,   43,   10,    9,  171,    1,    1],\n",
       "         [  12,   11,  184,  503,    4,  126,   54,   33, 1532,   67,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  41,   33,  164,    4,  616,  979,  141,  327,  212,  441,  133,  409,\n",
       "           738,  155,  217,  219,  220,  204,    1,    1],\n",
       "         [1313,    7,    2,  163,  196,  594,    5,  180,    4,    6,   15,  657,\n",
       "            34,   13,   10,    9,    1,    1,    1,    1],\n",
       "         [  96,  168,   59,    3,  464,  253,   55,   56,   16,   34,   99,    6,\n",
       "            50,  533,  198,    1,    1,    1,    1,    1],\n",
       "         [  64,   73,   68,  226,  235,   83,  782,  137,   70,  138,   74,   29,\n",
       "           299, 1123,  569, 1030,   20,    1,    1,    1],\n",
       "         [  19,   84,   26,  107,    4,   11,   24,   36,    5,   81,  222,   42,\n",
       "           790,  795,  157,  153,   28,    8,    1,    1],\n",
       "         [ 129, 2391,  245, 1115, 1419,  111,   16,   30,  125,   15,   80, 1724,\n",
       "            55,   56,   91,  248,   28,    8,    1,    1],\n",
       "         [  19,   84,   26,  107,   50,  597,    3,  351,   11,   24, 1539,  494,\n",
       "          1551,  142,    1,    1,    1,    1,    1,    1],\n",
       "         [  19,   84,  124,  107,   50, 1088,  309,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [  19,   86,   26,  122,  513,  254,  117,   95,  109,  158,  149,   89,\n",
       "           338,   23,  121,    1,    1,    1,    1,    1],\n",
       "         [  35, 2220, 1981, 2008,  792,   70,  138, 1280, 4079,   29,   51,   30,\n",
       "           120, 4004, 2618, 2681,   32,   22,    1,    1],\n",
       "         [  41,   33,  164,   51, 1065, 1095,    4,  210,   36,  162,  212,  658,\n",
       "          3163,  905,  133,   31,  587, 2332,    1,    1],\n",
       "         [   2,  590,  575,  990,  110,   68,  226,   17,  137,    4,  872,   12,\n",
       "            39, 2476,  119,  165, 2528,   20,    1,    1],\n",
       "         [  12,  730,  380,  826,   69,   15,   80, 2343,  657,    3,    6,  533,\n",
       "            99,   55,   56,   16,  458, 1085,    1,    1],\n",
       "         [  14,   47,  104,   31,   88,    5,    3,   82,   40,   58,  249,   75,\n",
       "           745,  172,   27,   81,  101,    8,    1,    1],\n",
       "         [  14,   47,  104,   48,  660,   31,  102, 1600,  611,   79,  120,   37,\n",
       "            23, 3005,   20,    1,    1,    1,    1,    1],\n",
       "         [  14,   47,  128,   31,  357,  102,  726,   48,  313,  370, 2147,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1]]),\n",
       " tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class SimpleLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=150, num_linear=3):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('./dataset/embeddings/w2v/w2v_title_300Epochs_1MinCount_9ContextWindow_150d.bin', binary=True)\n",
    "        weights = torch.FloatTensor(model.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        self.encoder_left = nn.LSTM(emb_dim, hidden_dim, num_layers=1)\n",
    "        self.encoder_right = nn.LSTM(emb_dim, hidden_dim, num_layers=1)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.mlp1 = nn.Linear(1296, 32)\n",
    "        self.predictor = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn_left, _ = self.encoder_left(self.embedding(seq[0]))\n",
    "        hdn_right, _ = self.encoder_right(self.embedding(seq[1]))\n",
    "        similarity = torch.matmul(hdn_left, torch.transpose(hdn_right, 1, 2))\n",
    "        similarity = torch.unsqueeze(similarity, 1)\n",
    "        x = self.conv1(similarity)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.predictor(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "em_sz = 150\n",
    "nh = 150\n",
    "nl = 3\n",
    "model = SimpleLSTMBaseline(nh, emb_dim=em_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: -6759.4239, Validation Loss: -24579.5024\n",
      "Epoch: 2, Training Loss: -57594.1496, Validation Loss: -99182.1950\n",
      "Epoch: 3, Training Loss: -163141.1378, Validation Loss: -237726.1536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-f8c8dc3375e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# turn on evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-35f3b2a911de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mhdn_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mhdn_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdn_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdn_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aml-project-EQl709OG/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for left, right, y in train_dl: # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "        preds = model([left, right])\n",
    "        loss = loss_func(torch.unsqueeze(y, 1), preds)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.data.item() * left.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dl)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for left, right, y in valid_dl:\n",
    "        preds = model([left, right])\n",
    "        loss = loss_func(torch.unsqueeze(y, 1), preds)\n",
    "        val_loss += loss.data.item() * left.size(0)\n",
    "\n",
    "    val_loss /= len(valid_dl)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "As = torch.randn(32,20,50)\n",
    "Bs = torch.randn(32,20,50)\n",
    "Cs = torch.matmul(As, torch.transpose(Bs, 1, 2))\n",
    "print(Cs.shape)\n",
    "print(torch.unsqueeze(Cs, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
